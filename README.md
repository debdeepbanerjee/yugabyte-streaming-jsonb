# YugabyteDB File Processor

A production-grade, highly scalable Spring Boot application that processes massive datasets from YugabyteDB and generates formatted files using BeanIO. Leverages Java 21 virtual threads for exceptional performance and memory efficiency, capable of processing billions of records with constant memory usage.

## Author

**Debdeep Banerjee**

---

## Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Technology Stack](#technology-stack)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Detailed Setup](#detailed-setup)
- [Configuration](#configuration)
- [Database Schema](#database-schema)
- [Processing Modes](#processing-modes)
- [File Formats](#file-formats)
- [Horizontal Scaling](#horizontal-scaling)
- [Performance Tuning](#performance-tuning)
- [Monitoring & Observability](#monitoring--observability)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)
- [Project Structure](#project-structure)
- [Best Practices](#best-practices)
- [Advanced Topics](#advanced-topics)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

The YugabyteDB File Processor is an enterprise-ready application designed to handle massive-scale data processing workloads. It provides three distinct processing modes:

1. **Standard Processing**: Traditional relational data with BeanIO file generation
2. **Enhanced Processing**: Relational data with extended attributes and file outputs
3. **Streaming JSONB Processing**: Memory-efficient processing of billions of JSONB records with constant O(fetch_size) memory complexity

The application implements distributed locking mechanisms, priority-based scheduling, and horizontal scalability, making it suitable for production environments processing millions to billions of records daily.

---

## Key Features

### Core Capabilities
- âœ… **Java 21 Virtual Threads**: All I/O operations leverage virtual threads for maximum scalability and non-blocking execution
- âœ… **YugabyteDB Integration**: Native PostgreSQL-compatible JDBC with optimized query patterns
- âœ… **Priority-Based Processing**: Configurable business center priorities drive intelligent job scheduling
- âœ… **Distributed Locking**: Database-level pessimistic locking with `FOR UPDATE SKIP LOCKED` prevents duplicate processing
- âœ… **Multiple Processing Modes**: Standard, Enhanced, and Streaming JSONB processing pipelines
- âœ… **BeanIO File Generation**: High-performance file writing with header/detail/trailer formats
- âœ… **Horizontal Scalability**: Run unlimited concurrent instances without conflicts
- âœ… **Memory Efficiency**: True streaming architecture processes billions of records with constant memory usage
- âœ… **Production Ready**: Comprehensive error handling, monitoring, and observability

### Streaming JSONB Features
- ğŸš€ **Constant Memory Usage**: Process billions of JSONB records with ~60MB RAM
- ğŸš€ **High Throughput**: 5,000-10,000 records/second sustained processing rates
- ğŸš€ **Cursor-Based Streaming**: JDBC ResultSet streaming with configurable fetch sizes
- ğŸš€ **Type-Safe Unmarshalling**: Jackson-based JSONB â†’ POJO conversion
- ğŸš€ **GIN Index Support**: Optimized JSONB queries with PostgreSQL GIN indexes
- ğŸš€ **Flexible Filtering**: Custom SQL queries and JSONB path-based filtering

### Enterprise Features
- ğŸ“Š **Monitoring**: Spring Boot Actuator with health checks and metrics
- ğŸ”’ **Fault Tolerance**: Stale lock detection and automatic recovery
- ğŸ“ˆ **Progress Tracking**: Real-time processing statistics and logging
- ğŸ”§ **Configurable**: Extensive YAML-based configuration for all aspects
- ğŸ§ª **Testable**: Comprehensive test suite with unit and integration tests

---

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Instance 1    â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”œâ”€â”€â”€â†’â”‚  YugabyteDB Cluster        â”‚
â”‚   Instance 2    â”‚â”€â”€â”¤    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â”‚ master_records       â”‚ â”‚
                     â”‚    â”‚  â”‚ detail_records       â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”‚ enhanced_detail_recs â”‚ â”‚
â”‚   Instance N    â”‚â”€â”€â”˜    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  (Hash Partitioned Tables) â”‚
        â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  File Outputs   â”‚
â”‚  - Standard     â”‚
â”‚  - Enhanced     â”‚
â”‚  - JSONB Stream â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Flow

1. **Job Discovery**: Each instance polls for available `master_id` records with `PENDING` status
2. **Priority Selection**: Records selected based on configurable business center priorities
3. **Distributed Locking**: Database-level row locking using `FOR UPDATE SKIP LOCKED`
4. **Processing Mode Selection**: Routes to Standard, Enhanced, or Streaming JSONB pipeline
5. **Data Streaming**: Detail records streamed from database with configurable fetch sizes
6. **File Writing**: BeanIO writes records to pipe-delimited files with headers and trailers
7. **Completion**: Lock released, status updated to `COMPLETED`, metrics recorded

### Streaming JSONB Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YugabyteDB - enhanced_detail_records           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Billions of rows with JSONB              â”‚   â”‚
â”‚ â”‚ Partitioned by master_id (Hash)          â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“ SELECT with cursor (fetch_size=1000)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JDBC ResultSet (Streaming Mode)                â”‚
â”‚ - Fetches N rows at a time                     â”‚
â”‚ - Keeps cursor open                            â”‚
â”‚ - Network buffer: ~10MB                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“ Stream<ResultSet> (lazy evaluation)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StreamingJsonbRepository.mapRow()              â”‚
â”‚ For each row:                                  â”‚
â”‚   1. Read JSONB as PGobject                    â”‚
â”‚   2. Extract JSON string                       â”‚
â”‚   3. Jackson unmarshal â†’ TransactionData       â”‚
â”‚ Memory: 1 object at a time + GC eligible      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“ Stream<EnhancedDetailRecord>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StreamingJsonbFileWriter.flattenRecord()        â”‚
â”‚ For each record:                               â”‚
â”‚   1. Extract nested JSONB fields               â”‚
â”‚   2. Flatten to output structure               â”‚
â”‚   3. Create EnhancedDetailOutput               â”‚
â”‚ Memory: 1 flattened object per iteration       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“ EnhancedDetailOutput
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BeanIO Writer (Buffered)                       â”‚
â”‚   1. Convert to pipe-delimited string          â”‚
â”‚   2. Write to 32KB buffer                      â”‚
â”‚   3. Flush to disk when buffer full            â”‚
â”‚ Memory: 32KB buffer only                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output File                                    â”‚
â”‚ HEADER|master_id|business_center|date|count    â”‚
â”‚ DETAIL|1|...|extracted_jsonb_fields            â”‚
â”‚ DETAIL|2|...|extracted_jsonb_fields            â”‚
â”‚ ...                                            â”‚
â”‚ TRAILER|total_records|total_amount             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Memory Complexity**: O(fetch_size) - constant regardless of total record count

---

## Technology Stack

### Core Technologies
- **Java**: 21 (Virtual Threads, Preview Features)
- **Spring Boot**: 3.2.2
- **Database**: YugabyteDB (PostgreSQL 11+ compatible)
- **File Processing**: BeanIO 3.2.0
- **Build Tool**: Gradle 8.x with Kotlin DSL
- **JDBC**: Direct JDBC with JdbcTemplate (non-blocking via virtual threads)

### Key Dependencies
- **PostgreSQL JDBC Driver**: 42.7.1 (YugabyteDB compatible)
- **Jackson**: 2.16.1 (JSONB unmarshalling, JSR-310 support)
- **Lombok**: 1.18.30 (Boilerplate reduction)
- **Logback**: Classic logging framework
- **Spring Boot Actuator**: Monitoring and health checks
- **JUnit 5**: Testing framework

### Build System
- Gradle with Kotlin DSL
- Custom build logic plugins
- Jacoco for code coverage
- Spotless for code formatting
- Detekt for Kotlin static analysis

---

## Prerequisites

### Required
- **Java Development Kit (JDK)**: 21 or higher
- **YugabyteDB**: Latest version (or PostgreSQL 11+)
- **Gradle**: 8.x or higher (or use included wrapper)

### Optional
- **Docker**: For containerized YugabyteDB deployment
- **Docker Compose**: For orchestrated multi-container setup
- **PgAdmin**: For database management (included in Docker Compose)

### System Requirements
- **Memory**: Minimum 2GB RAM (4GB+ recommended)
- **Disk**: 10GB free space (more for large datasets)
- **CPU**: 2+ cores (4+ recommended for optimal performance)

---

## Quick Start

Get the application running in under 5 minutes:

### 1. Start YugabyteDB with Docker Compose

```bash
docker-compose up -d
```

This starts:
- YugabyteDB on port 5433 (YSQL/PostgreSQL-compatible)
- YugabyteDB Master UI on port 7000
- YugabyteDB TServer UI on port 9000
- PgAdmin on port 5050 (optional database management)

### 2. Initialize the Database

```bash
# Wait for YugabyteDB to be ready (~30 seconds)
sleep 30

# Connect and run schema
docker exec -i yugabyte-db bin/ysqlsh -h localhost -U yugabyte -d yugabyte < src/main/resources/schema.sql

# Generate test data
docker exec -i yugabyte-db bin/ysqlsh -h localhost -U yugabyte -d yugabyte < src/main/resources/test-data.sql
```

### 3. Build and Run the Application

```bash
# Build
./gradlew clean build

# Run
./gradlew bootRun
```

### 4. Monitor Progress

```bash
# Watch logs
tail -f logs/application.log

# Or check database
docker exec -it yugabyte-db bin/ysqlsh -h localhost -U yugabyte -d yugabyte

# Check processing status
SELECT * FROM processing_status;
```

### 5. View Generated Files

```bash
ls -lh output/
cat output/NYC_1_*.txt
```

---

## Detailed Setup

### Option 1: Using Docker (Recommended)

#### Start YugabyteDB
```bash
docker run -d --name yugabyte \
  -p 5433:5433 \
  -p 7000:7000 \
  -p 9000:9000 \
  yugabytedb/yugabyte:latest \
  bin/yugabyted start --daemon=false
```

#### Initialize Database
```bash
# Connect to YugabyteDB
docker exec -it yugabyte bin/ysqlsh -h localhost -U yugabyte -d yugabyte

# Run schema
\i src/main/resources/schema.sql

# Load test data (optional)
\i src/main/resources/test-data.sql
```

### Option 2: Local YugabyteDB Installation

#### Download and Install
```bash
# Download from https://docs.yugabyte.com/preview/quick-start/
wget https://downloads.yugabyte.com/releases/latest/yugabyte-latest.tar.gz
tar -xvf yugabyte-latest.tar.gz
cd yugabyte-*

# Start YugabyteDB
./bin/yugabyted start
```

#### Initialize Database
```bash
# Connect via psql
./bin/ysqlsh -h localhost -p 5433 -U yugabyte -d yugabyte

# Run schema
\i /path/to/src/main/resources/schema.sql
```

### Building the Application

```bash
# Clean and build
./gradlew clean build

# Skip tests (faster)
./gradlew clean build -x test

# Build with verbose output
./gradlew clean build --info
```

### Running the Application

#### Using Gradle
```bash
# Default configuration
./gradlew bootRun

# Custom port
./gradlew bootRun --args='--server.port=8081'

# With JVM options
./gradlew bootRun --args='--server.port=8081 -Xmx4g'
```

#### Using JAR
```bash
# Build JAR
./gradlew bootJar

# Run JAR
java -jar build/libs/yugabyte-file-processor-1.0.0.jar

# With custom configuration
java -jar build/libs/yugabyte-file-processor-1.0.0.jar \
  --spring.datasource.url=jdbc:postgresql://localhost:5433/yugabyte \
  --server.port=8080
```

---

## Configuration

### Application Configuration (application.yml)

```yaml
spring:
  application:
    name: yugabyte-file-processor
  
  datasource:
    url: jdbc:postgresql://localhost:5433/yugabyte
    username: yugabyte
    password: yugabyte
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20       # Max connections
      minimum-idle: 5              # Min idle connections
      connection-timeout: 30000    # 30 seconds
      idle-timeout: 600000         # 10 minutes
      max-lifetime: 1800000        # 30 minutes

  threads:
    virtual:
      enabled: true                # Enable Java 21 virtual threads

server:
  port: 8080

processor:
  # Priority configuration - higher number = higher priority
  business-center-priorities:
    NYC: 100                       # Highest priority
    LON: 90
    TKY: 80
    HKG: 70
    SIN: 60
    CHI: 50
    LAX: 40
    SFO: 30
    BOS: 20
    MIA: 10
  
  # Processing configuration
  batch-size: 1000                 # JDBC fetch size for streaming
  lock-timeout-seconds: 300        # Stale lock timeout (5 minutes)
  poll-interval-seconds: 5         # Poll interval when no work
  max-concurrent-masters: 10       # Max concurrent master processing
  output-directory: ./output       # Output file directory

logging:
  level:
    com.example: DEBUG
    org.springframework: INFO
```

### Configuration Reference

#### Database Configuration

| Property | Description | Default | Valid Range |
|----------|-------------|---------|-------------|
| `spring.datasource.url` | JDBC URL for YugabyteDB | `jdbc:postgresql://localhost:5433/yugabyte` | Valid JDBC URL |
| `spring.datasource.username` | Database username | `yugabyte` | - |
| `spring.datasource.password` | Database password | `yugabyte` | - |
| `spring.datasource.hikari.maximum-pool-size` | Max connection pool size | `20` | 1-1000 |
| `spring.datasource.hikari.minimum-idle` | Min idle connections | `5` | 1-maximum-pool-size |
| `spring.datasource.hikari.connection-timeout` | Connection timeout (ms) | `30000` | 1000+ |

#### Processing Configuration

| Property | Description | Default | Valid Range | Impact |
|----------|-------------|---------|-------------|--------|
| `processor.batch-size` | JDBC fetch size | `1000` | 100-10000 | Higher = more memory, better throughput |
| `processor.lock-timeout-seconds` | Stale lock timeout | `300` | 60-3600 | Time before locks expire |
| `processor.poll-interval-seconds` | Poll interval | `5` | 1-60 | Frequency of polling for work |
| `processor.max-concurrent-masters` | Max concurrent processing | `10` | 1-100 | Number of parallel jobs |
| `processor.output-directory` | Output directory | `./output` | Valid path | Where files are written |

#### Business Center Priorities

Define priorities for business centers. Higher values = higher priority. Configure based on your business requirements:

```yaml
processor:
  business-center-priorities:
    CRITICAL_CENTER: 100
    HIGH_PRIORITY: 80
    MEDIUM_PRIORITY: 50
    LOW_PRIORITY: 20
```

---

## Database Schema

### Tables Overview

The application uses three main tables:

1. **master_records**: Job control and locking
2. **detail_records**: Standard relational detail data
3. **enhanced_detail_records**: Enhanced records with JSONB column

### master_records

Stores master batch information and locking state for distributed processing.

```sql
CREATE TABLE master_records (
    master_id BIGSERIAL PRIMARY KEY,
    business_center_code VARCHAR(10) NOT NULL,
    priority INT DEFAULT 0,
    status VARCHAR(20) NOT NULL DEFAULT 'PENDING',
    locked_by VARCHAR(255),
    locked_at TIMESTAMP,
    error_message TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP
);
```

**Columns**:
- `master_id`: Unique identifier (auto-incrementing)
- `business_center_code`: Business unit identifier for priority routing
- `priority`: Processing priority (higher = more urgent)
- `status`: Current state (`PENDING`, `PROCESSING`, `COMPLETED`, `FAILED`)
- `locked_by`: Instance ID that acquired the lock
- `locked_at`: Timestamp when lock was acquired
- `error_message`: Error details if processing failed
- `created_at`: Record creation timestamp
- `updated_at`: Last update timestamp

**Indexes**:
```sql
-- Priority-based selection
CREATE INDEX idx_master_priority_status 
    ON master_records(priority DESC, created_at ASC) 
    WHERE status = 'PENDING';

-- Business center queries
CREATE INDEX idx_master_business_center 
    ON master_records(business_center_code);

-- Lock management
CREATE INDEX idx_master_locks 
    ON master_records(locked_by, locked_at) 
    WHERE locked_by IS NOT NULL;
```

### detail_records

Stores standard detail records. Hash-partitioned for horizontal scalability.

```sql
CREATE TABLE detail_records (
    detail_id BIGSERIAL,
    master_id BIGINT NOT NULL,
    record_type VARCHAR(20) NOT NULL,
    account_number VARCHAR(50),
    customer_name VARCHAR(200),
    amount DECIMAL(18, 2),
    currency VARCHAR(3),
    description TEXT,
    transaction_date TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (master_id, detail_id)
) PARTITION BY HASH (master_id);
```

**Partitioning**: 4 hash partitions for distributed storage and parallel queries.

### enhanced_detail_records

Stores enhanced records with JSONB column for complex, nested data structures.

```sql
CREATE TABLE enhanced_detail_records (
    detail_id BIGSERIAL,
    master_id BIGINT NOT NULL,
    record_type VARCHAR(20) NOT NULL,
    account_number VARCHAR(50),
    customer_name VARCHAR(200),
    amount DECIMAL(18, 2),
    currency VARCHAR(3),
    description TEXT,
    transaction_date TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- JSONB column for complex transaction data
    transaction_data JSONB,
    
    processing_status VARCHAR(20) DEFAULT 'PENDING',
    error_message TEXT,
    
    PRIMARY KEY (master_id, detail_id)
) PARTITION BY HASH (master_id);
```

**JSONB Indexes**:
```sql
-- Full JSONB GIN index for containment queries
CREATE INDEX idx_enhanced_detail_jsonb 
    ON enhanced_detail_records USING GIN (transaction_data);

-- Specific path indexes for common queries
CREATE INDEX idx_enhanced_detail_customer_id 
    ON enhanced_detail_records ((transaction_data->>'customer_id'));

CREATE INDEX idx_enhanced_detail_status 
    ON enhanced_detail_records ((transaction_data->>'status'));

CREATE INDEX idx_enhanced_detail_risk_score 
    ON enhanced_detail_records (((transaction_data->>'risk_score')::NUMERIC));
```

### Database Functions

#### cleanup_stale_locks()

Automatically recovers from stale locks caused by application crashes or network failures.

```sql
-- Usage
SELECT cleanup_stale_locks(300);  -- Clean locks older than 5 minutes
```

### Database Views

#### processing_status

Real-time view of processing statistics.

```sql
-- Query processing status
SELECT * FROM processing_status;

-- Example output:
--  status     | record_count | business_centers | active_instances 
-- ------------+--------------+------------------+------------------
--  PENDING    |         1450 |                8 |                0
--  PROCESSING |           25 |                5 |                3
--  COMPLETED  |         3210 |               10 |                0
--  FAILED     |           12 |                2 |                0
```

---

## Processing Modes

The application supports three distinct processing modes:

### 1. Standard Processing

**Use Case**: Traditional relational data with simple structure

**Data Source**: `detail_records` table

**Output Format**: Pipe-delimited file with header/trailer

**Example**:
```
HEADER|12345|NYC|20250213|1000
TXN|1|ACC0001234567|John Doe|1500.50|USD|Payment received|20250209143022
TXN|2|ACC0007654321|Jane Smith|2300.75|USD|Transfer out|20250209143155
...
TRAILER|1000|1525750.50
```

**Configuration**: Default mode, enabled by default

### 2. Enhanced Processing

**Use Case**: Relational data with extended attributes

**Data Source**: `enhanced_detail_records` table (relational fields only)

**Output Format**: Enhanced pipe-delimited format with additional fields

**Features**:
- Extended metadata fields
- Additional validation rules
- Custom file naming patterns

### 3. Streaming JSONB Processing

**Use Case**: Massive datasets with complex JSONB structures

**Data Source**: `enhanced_detail_records` table (with JSONB column)

**Key Characteristics**:
- **Memory Efficiency**: O(fetch_size) memory usage, constant regardless of total rows
- **High Throughput**: 5,000-10,000 records/second
- **Type-Safe**: Jackson unmarshalling to Java POJOs
- **Lazy Evaluation**: True streaming with cursor-based fetching

**Output Format**: Flattened JSONB fields in pipe-delimited format

**Example Input (JSONB)**:
```json
{
  "customer": {
    "id": "C12345",
    "email": "john.doe@example.com",
    "tier": "GOLD"
  },
  "merchant": {
    "name": "Amazon",
    "category": "E-Commerce",
    "country": "US"
  },
  "items": [
    {"product": "Laptop", "price": 1200.00},
    {"product": "Mouse", "price": 25.00}
  ],
  "status": "COMPLETED",
  "risk_score": 15.5
}
```

**Example Output (Flattened)**:
```
DETAIL|1|ACC001|John Doe|1225.00|USD|...|john.doe@example.com|Amazon|2|COMPLETED|15.5
```

**Memory Analysis**:
```
Total Memory = (fetch_size Ã— avg_row_size) + buffer_size + JVM_overhead

Example with fetch_size=1000, avg_row_size=10KB:
Total = (1000 Ã— 10KB) + 32KB + 50MB â‰ˆ 60MB

This is CONSTANT regardless of total row count!
```

---

## File Formats

### Standard File Format

```
HEADER|{master_id}|{business_center_code}|{date}|{record_count}
{record_type}|{detail_id}|{account_number}|{customer_name}|{amount}|{currency}|{description}|{transaction_date}
{record_type}|{detail_id}|{account_number}|{customer_name}|{amount}|{currency}|{description}|{transaction_date}
...
TRAILER|{total_records}|{total_amount}
```

**Example**:
```
HEADER|12345|NYC|20250213|3
TXN|1|ACC0001234567|John Doe|1500.50|USD|Payment received|20250209143022
TXN|2|ACC0007654321|Jane Smith|2300.75|USD|Transfer out|20250209143155
TXN|3|ACC0009876543|Bob Johnson|500.00|USD|Deposit|20250209144301
TRAILER|3|4301.25
```

### Enhanced File Format

```
HEADER|{master_id}|{business_center_code}|{date}|{record_count}|{additional_metadata}
{record_type}|{detail_id}|{account}|{customer}|{amount}|{currency}|{desc}|{date}|{extended_fields}
...
TRAILER|{total_records}|{total_amount}|{summary_stats}
```

### Streaming JSONB File Format

```
HEADER|{master_id}|{business_center_code}|{date}|{record_count}
DETAIL|{detail_id}|{account}|{customer}|{amount}|{currency}|{desc}|{date}|{customer_email}|{merchant_name}|{items_count}|{status}|{risk_score}
...
TRAILER|{total_records}|{total_amount}
```

### File Naming Conventions

- **Standard**: `{business_center}_{master_id}_{timestamp}.txt`
- **Enhanced**: `{business_center}_{master_id}_enhanced_{timestamp}.txt`
- **Streaming**: `{business_center}_{master_id}_jsonb_{timestamp}.txt`

**Examples**:
```
NYC_12345_20250213_143022.txt
LON_67890_enhanced_20250213_143155.txt
TKY_11111_jsonb_20250213_144301.txt
```

---

## Locking Mechanism

The application uses PostgreSQL's `FOR UPDATE SKIP LOCKED` to implement distributed, database-level locking without contention.

### Lock Acquisition Query

```sql
SELECT master_id
FROM master_records
WHERE status = 'PENDING'
  AND (locked_by IS NULL 
       OR locked_at < NOW() - INTERVAL '300 seconds')
ORDER BY priority DESC, created_at ASC
LIMIT 1
FOR UPDATE SKIP LOCKED
```

### Benefits

âœ… **No Contention**: Instances skip locked rows automatically
âœ… **Automatic Recovery**: Stale locks expire after configurable timeout
âœ… **Database-Enforced**: Consistency guaranteed by database engine
âœ… **Horizontal Scaling**: Unlimited concurrent instances without conflicts
âœ… **Prioritization**: Higher priority jobs processed first

### Lock Lifecycle

1. **Acquisition**: Instance attempts to lock next available `master_id`
2. **Processing**: Record marked as `PROCESSING`, `locked_by` set to instance ID
3. **Completion**: Status updated to `COMPLETED`, lock released
4. **Failure**: Status set to `FAILED`, error message recorded, lock released
5. **Timeout**: Stale locks automatically recovered after `lock-timeout-seconds`

### Stale Lock Recovery

```sql
-- Manual cleanup
UPDATE master_records
SET locked_by = NULL, 
    locked_at = NULL, 
    status = 'PENDING',
    updated_at = CURRENT_TIMESTAMP
WHERE locked_at < NOW() - INTERVAL '5 minutes'
  AND status = 'PROCESSING';

-- Using built-in function
SELECT cleanup_stale_locks(300);  -- 300 seconds = 5 minutes
```

---

## Horizontal Scaling

The application is designed for horizontal scalability from the ground up.

### Running Multiple Instances

#### Using Gradle
```bash
# Terminal 1
./gradlew bootRun

# Terminal 2
./gradlew bootRun --args='--server.port=8081'

# Terminal 3
./gradlew bootRun --args='--server.port=8082'
```

#### Using JAR
```bash
# Instance 1
java -jar build/libs/yugabyte-file-processor-1.0.0.jar --server.port=8080

# Instance 2
java -jar build/libs/yugabyte-file-processor-1.0.0.jar --server.port=8081

# Instance 3
java -jar build/libs/yugabyte-file-processor-1.0.0.jar --server.port=8082
```

### Instance Behavior

Each instance will:
- Generate a unique instance ID (hostname + UUID)
- Independently poll for available work
- Acquire locks on different `master_id` records
- Process work concurrently without conflicts
- Share no state except via the database

### Load Distribution

Work is distributed based on:
1. **Priority**: Higher priority business centers processed first
2. **Age**: Older records processed before newer ones (within same priority)
3. **Availability**: Only unlocked records are considered

### Monitoring Multiple Instances

```sql
-- View active instances
SELECT DISTINCT locked_by
FROM master_records
WHERE locked_by IS NOT NULL;

-- View work distribution
SELECT 
    locked_by, 
    COUNT(*) as processing_count,
    STRING_AGG(DISTINCT business_center_code, ', ') as business_centers
FROM master_records
WHERE status = 'PROCESSING'
GROUP BY locked_by;
```

---

## Performance Tuning

### For Standard/Enhanced Processing

#### 1. Adjust Batch Size
```yaml
processor:
  batch-size: 5000  # Larger fetch size = fewer round-trips
```

**Guidelines**:
- Small datasets (<10K): 100-500
- Medium datasets (10K-1M): 1000-2000
- Large datasets (>1M): 2000-5000

#### 2. Increase Concurrent Processing
```yaml
processor:
  max-concurrent-masters: 50  # More parallel jobs
```

**Guidelines**:
- Match to available CPU cores
- Balance with connection pool size
- Monitor system resources

#### 3. Tune Connection Pool
```yaml
spring:
  datasource:
    hikari:
      maximum-pool-size: 50      # Match or exceed max-concurrent-masters
      minimum-idle: 10
```

#### 4. Optimize Database
```sql
-- Ensure indexes exist
ANALYZE master_records;
ANALYZE detail_records;

-- Vacuum regularly
VACUUM ANALYZE master_records;
VACUUM ANALYZE detail_records;
```

### For Streaming JSONB Processing

#### Memory vs. Throughput Trade-off

| Fetch Size | Memory | Throughput | Use Case |
|-----------|--------|------------|----------|
| 100 | ~1MB | Low | Memory-constrained environments |
| 1,000 | ~10MB | Medium | Balanced (recommended default) |
| 5,000 | ~50MB | High | High-throughput requirements |
| 10,000 | ~100MB | Very High | Maximum speed with ample memory |

#### Configuration Example
```yaml
processor:
  batch-size: 5000                # Fetch size for streaming
  max-concurrent-masters: 20      # Parallel streaming jobs
  
spring:
  datasource:
    hikari:
      maximum-pool-size: 30       # Connection pool
      connection-timeout: 60000   # Longer timeout for large streams
```

#### Database Optimization
```sql
-- Enable parallel queries
SET max_parallel_workers_per_gather = 4;

-- Optimize JSONB queries
SET enable_seqscan = ON;
SET enable_indexscan = ON;
SET enable_bitmapscan = ON;

-- No timeouts for long-running streams
SET statement_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
```

### JVM Tuning

```bash
# For Standard/Enhanced Processing
java -Xms2g -Xmx4g -jar yugabyte-file-processor-1.0.0.jar

# For Streaming JSONB (minimal heap needed)
java -Xms512m -Xmx2g -jar yugabyte-file-processor-1.0.0.jar

# With GC logging
java -Xmx4g \
  -Xlog:gc*:file=gc.log:time,uptime,level,tags \
  -jar yugabyte-file-processor-1.0.0.jar
```

### File I/O Optimization

#### 1. Enable Compression (for large files)
```java
// In FileWriterService
GZIPOutputStream gzipStream = new GZIPOutputStream(
    Files.newOutputStream(outputPath)
);
```

#### 2. Increase Buffer Size
```java
// In FileWriterService
BufferedWriter writer = new BufferedWriter(
    new FileWriter(output), 
    65536  // 64KB buffer
);
```

---

## Monitoring & Observability

### Spring Boot Actuator Endpoints

```bash
# Health check
curl http://localhost:8080/actuator/health

# Metrics
curl http://localhost:8080/actuator/metrics

# Info
curl http://localhost:8080/actuator/info

# Specific metric
curl http://localhost:8080/actuator/metrics/jvm.memory.used
```

### Database Monitoring

#### Processing Status
```sql
-- Overall status
SELECT * FROM processing_status;

-- Detailed status
SELECT 
    status,
    business_center_code,
    COUNT(*) as count,
    MIN(created_at) as oldest,
    MAX(created_at) as newest
FROM master_records
GROUP BY status, business_center_code
ORDER BY status, business_center_code;
```

#### Active Locks
```sql
-- Current locks
SELECT 
    master_id, 
    business_center_code,
    locked_by, 
    locked_at,
    EXTRACT(EPOCH FROM (NOW() - locked_at)) as lock_age_seconds
FROM master_records
WHERE locked_by IS NOT NULL
ORDER BY locked_at;
```

#### Performance Metrics
```sql
-- Processing throughput
SELECT 
    DATE_TRUNC('hour', updated_at) as hour,
    COUNT(*) as completed_count,
    COUNT(*) / 60.0 as per_minute
FROM master_records
WHERE status = 'COMPLETED'
  AND updated_at > NOW() - INTERVAL '24 hours'
GROUP BY DATE_TRUNC('hour', updated_at)
ORDER BY hour DESC;
```

### Application Logs

#### Log Levels
```yaml
logging:
  level:
    com.example: DEBUG                          # Application logs
    com.example.service: INFO                   # Service layer
    com.example.repository: DEBUG               # SQL queries
    org.springframework.jdbc: DEBUG             # JDBC operations
    org.springframework: INFO                   # Spring framework
```

#### Key Log Messages
- `Starting streaming JSONB read`: Streaming begins
- `Progress for master_id`: Processing progress updates
- `Completed streaming write`: Processing complete
- `No work available`: Polling found no pending records
- `Lock acquisition failed`: Another instance locked the record

### JVM Monitoring

```bash
# Heap usage
jstat -gc <pid> 1000

# Thread dump
jstack <pid> > threads.txt

# Heap dump
jmap -dump:live,format=b,file=heap.bin <pid>

# JVM flags
jinfo -flags <pid>
```

### Performance Testing

```sql
-- Insert large test dataset
INSERT INTO master_records (business_center_code, priority, status)
SELECT 
    CASE (random() * 5)::INT
        WHEN 0 THEN 'NYC'
        WHEN 1 THEN 'LON'
        WHEN 2 THEN 'TKY'
        WHEN 3 THEN 'HKG'
        ELSE 'SIN'
    END,
    (random() * 100)::INT,
    'PENDING'
FROM generate_series(1, 1000);

-- Generate detail records (1000 per master)
INSERT INTO detail_records (
    master_id, record_type, account_number, customer_name, 
    amount, currency, description, transaction_date
)
SELECT 
    m.master_id,
    'TXN',
    'ACC' || LPAD((random() * 10000000)::INT::TEXT, 10, '0'),
    'Customer ' || (random() * 1000)::INT,
    (random() * 10000)::DECIMAL(18,2),
    'USD',
    'Transaction ' || gs,
    CURRENT_TIMESTAMP - (random() * INTERVAL '365 days')
FROM master_records m
CROSS JOIN generate_series(1, 1000) gs
WHERE m.status = 'PENDING';
```

---

## Testing

### Unit Tests

```bash
# Run all tests
./gradlew test

# Run specific test class
./gradlew test --tests ProcessorConfigPropertiesTest

# Run with coverage
./gradlew test jacocoTestReport
```

### Integration Tests

```bash
# Run integration tests (requires database)
./gradlew integrationTest

# With Testcontainers
./gradlew test --tests '*IT'
```

### Manual Testing

#### 1. Insert Test Data
```sql
-- Connect to database
docker exec -it yugabyte-db bin/ysqlsh -h localhost -U yugabyte -d yugabyte

-- Load test data script
\i src/main/resources/test-data.sql
```

#### 2. Start Application
```bash
./gradlew bootRun
```

#### 3. Monitor Processing
```bash
# Watch logs
tail -f logs/application.log

# Check database
docker exec -it yugabyte-db bin/ysqlsh -h localhost -U yugabyte -d yugabyte \
  -c "SELECT * FROM processing_status;"
```

#### 4. Verify Output
```bash
# List generated files
ls -lh output/

# View file content
cat output/NYC_1_*.txt
```

### Test Data Generation

#### Small Dataset (for quick testing)
```sql
-- 10 masters, 100 details each
INSERT INTO master_records (business_center_code, priority, status)
SELECT 'NYC', 100, 'PENDING' FROM generate_series(1, 10);

INSERT INTO detail_records (master_id, record_type, account_number, customer_name, amount, currency, description, transaction_date)
SELECT m.master_id, 'TXN', 'ACC' || gs, 'Customer ' || gs, 
       (random() * 1000)::DECIMAL(18,2), 'USD', 'Txn ' || gs, NOW()
FROM master_records m
CROSS JOIN generate_series(1, 100) gs
WHERE m.status = 'PENDING';
```

#### Large Dataset (for performance testing)
```sql
-- 1000 masters, 10000 details each = 10M records
INSERT INTO master_records (business_center_code, priority, status)
SELECT 
    CASE (random() * 5)::INT
        WHEN 0 THEN 'NYC' WHEN 1 THEN 'LON' WHEN 2 THEN 'TKY'
        WHEN 3 THEN 'HKG' ELSE 'SIN'
    END,
    (random() * 100)::INT,
    'PENDING'
FROM generate_series(1, 1000);

INSERT INTO detail_records (master_id, record_type, account_number, customer_name, amount, currency, description, transaction_date)
SELECT 
    m.master_id, 'TXN',
    'ACC' || LPAD((random() * 10000000)::INT::TEXT, 10, '0'),
    'Customer ' || (random() * 10000)::INT,
    (random() * 10000)::DECIMAL(18,2),
    'USD',
    'Transaction ' || gs,
    CURRENT_TIMESTAMP - (random() * INTERVAL '365 days')
FROM master_records m
CROSS JOIN generate_series(1, 10000) gs
WHERE m.status = 'PENDING';
```

---

## Troubleshooting

### Common Issues

#### Issue: No records being processed

**Symptoms**: Application runs but no files generated

**Diagnosis**:
```sql
-- Check for pending records
SELECT COUNT(*) FROM master_records WHERE status = 'PENDING';

-- Check priorities match configuration
SELECT DISTINCT business_center_code FROM master_records WHERE status = 'PENDING';
```

**Solutions**:
1. Verify records exist with `status = 'PENDING'`
2. Check database connection in logs
3. Ensure priority configuration matches business center codes in data
4. Verify `poll-interval-seconds` is reasonable

#### Issue: Stale locks preventing processing

**Symptoms**: Records stuck in `PROCESSING` state

**Diagnosis**:
```sql
-- Find stale locks
SELECT 
    master_id, 
    business_center_code,
    locked_by, 
    locked_at,
    EXTRACT(EPOCH FROM (NOW() - locked_at)) / 60 as minutes_locked
FROM master_records
WHERE locked_by IS NOT NULL
  AND locked_at < NOW() - INTERVAL '10 minutes';
```

**Solutions**:
```sql
-- Clean up stale locks
SELECT cleanup_stale_locks(300);

-- Or manually
UPDATE master_records
SET locked_by = NULL, 
    locked_at = NULL, 
    status = 'PENDING'
WHERE locked_at < NOW() - INTERVAL '5 minutes'
  AND status = 'PROCESSING';
```

#### Issue: OutOfMemoryError

**Symptoms**: Application crashes with OOM error

**Diagnosis**:
```bash
# Check heap usage
jmap -heap <pid>

# Memory dump for analysis
jmap -dump:live,format=b,file=heap.bin <pid>
```

**Solutions**:
1. Reduce `batch-size`: `batch-size: 500`
2. Increase heap: `java -Xmx4g -jar app.jar`
3. Verify streams are closed properly
4. Check for resource leaks (unclosed connections, streams)

#### Issue: Slow performance

**Symptoms**: Processing taking longer than expected

**Diagnosis**:
```sql
-- Check if indexes are used
EXPLAIN ANALYZE
SELECT * FROM detail_records WHERE master_id = 123;

-- Should show "Index Scan" not "Seq Scan"
```

**Solutions**:
1. Verify indexes exist: `\di` in psql
2. Run `ANALYZE` on tables
3. Increase `batch-size` for higher throughput
4. Check network latency between app and database
5. Enable parallel queries in PostgreSQL

#### Issue: Database connection failures

**Symptoms**: `Connection refused` or timeout errors

**Diagnosis**:
```bash
# Check YugabyteDB is running
docker ps | grep yugabyte

# Check connectivity
telnet localhost 5433

# View YugabyteDB logs
docker logs yugabyte-db
```

**Solutions**:
1. Ensure YugabyteDB is running: `docker-compose up -d`
2. Verify port 5433 is accessible
3. Check firewall rules
4. Verify JDBC URL in configuration

#### Issue: Files not appearing in output directory

**Symptoms**: Processing completes but no files created

**Diagnosis**:
```bash
# Check output directory exists and is writable
ls -la ./output

# Check application logs for file writing errors
grep -i "error.*writing" logs/application.log
```

**Solutions**:
1. Create output directory: `mkdir -p output`
2. Verify write permissions: `chmod 755 output`
3. Check `processor.output-directory` configuration
4. Review application logs for exceptions

---

## Project Structure

```
yugabyte-file-processor/
â”œâ”€â”€ build-logic/                    # Custom Gradle build logic
â”‚   â”œâ”€â”€ conventions/                # Gradle convention plugins
â”‚   â”œâ”€â”€ junit/                      # JUnit configuration
â”‚   â”œâ”€â”€ kotlin-core/                # Kotlin build configuration
â”‚   â”œâ”€â”€ meta/                       # Meta build plugins
â”‚   â”œâ”€â”€ settings/                   # Settings plugins
â”‚   â”œâ”€â”€ spring/                     # Spring Boot plugins
â”‚   â””â”€â”€ test-fixtures/              # Test fixtures
â”œâ”€â”€ gradle/
â”‚   â”œâ”€â”€ libs.versions.toml          # Dependency versions catalog
â”‚   â””â”€â”€ wrapper/                    # Gradle wrapper
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ java/com/example/
â”‚   â”‚   â”‚   â”œâ”€â”€ YugabyteFileProcessorApplication.java
â”‚   â”‚   â”‚   â”œâ”€â”€ beanio/             # BeanIO record definitions
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FileHeader.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FileTrailer.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedFileHeader.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedFileTrailer.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ EnhancedDetailOutput.java
â”‚   â”‚   â”‚   â”œâ”€â”€ config/             # Application configuration
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessorConfigProperties.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ VirtualThreadConfig.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ JacksonConfig.java
â”‚   â”‚   â”‚   â”œâ”€â”€ model/              # Domain models
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MasterRecord.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DetailRecord.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedDetailRecord.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TransactionData.java
â”‚   â”‚   â”‚   â”œâ”€â”€ repository/         # Data access layer
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MasterRecordRepository.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DetailRecordRepository.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedDetailRecordRepository.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ StreamingJsonbRepository.java
â”‚   â”‚   â”‚   â”œâ”€â”€ service/            # Business logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RecordProcessingService.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FileWriterService.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedRecordProcessingService.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EnhancedFileWriterService.java
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ StreamingJsonbProcessingService.java
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ StreamingJsonbFileWriter.java
â”‚   â”‚   â”‚   â””â”€â”€ scheduler/          # Scheduled tasks
â”‚   â”‚   â”‚       â””â”€â”€ ProcessingScheduler.java
â”‚   â”‚   â””â”€â”€ resources/
â”‚   â”‚       â”œâ”€â”€ application.yml                # Main configuration
â”‚   â”‚       â”œâ”€â”€ beanio-mapping.xml             # Standard BeanIO mapping
â”‚   â”‚       â”œâ”€â”€ beanio-mapping-enhanced.xml    # Enhanced BeanIO mapping
â”‚   â”‚       â”œâ”€â”€ schema.sql                     # Database schema
â”‚   â”‚       â”œâ”€â”€ test-data.sql                  # Test data generation
â”‚   â”‚       â”œâ”€â”€ test-data-enhanced.sql         # Enhanced test data
â”‚   â”‚       â”œâ”€â”€ jsonb-example.sql              # JSONB data examples
â”‚   â”‚       â””â”€â”€ streaming-performance-test.sql # Performance testing
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/com/example/
â”‚           â”œâ”€â”€ repository/
â”‚           â”‚   â””â”€â”€ StreamingJsonbRepositoryTest.java
â”‚           â””â”€â”€ service/
â”‚               â”œâ”€â”€ ProcessorConfigPropertiesTest.java
â”‚               â””â”€â”€ TransactionDataTest.java
â”œâ”€â”€ build.gradle.kts                # Main build script
â”œâ”€â”€ settings.gradle.kts             # Gradle settings
â”œâ”€â”€ docker-compose.yml              # Docker Compose configuration
â”œâ”€â”€ QUICKSTART.md                   # Quick start guide
â”œâ”€â”€ STREAMING_JSONB_GUIDE.md        # Streaming JSONB documentation
â””â”€â”€ README.md                       # This file
```

---

## Best Practices

### Code Quality
- âœ… Use Lombok for boilerplate reduction
- âœ… Follow Spring Boot conventions
- âœ… Implement proper exception handling
- âœ… Add comprehensive logging
- âœ… Write unit and integration tests
- âœ… Use meaningful variable and method names
- âœ… Document complex logic with comments

### Database Operations
- âœ… Always use connection pooling (HikariCP)
- âœ… Close ResultSets and Statements properly
- âœ… Use prepared statements to prevent SQL injection
- âœ… Leverage database indexes for performance
- âœ… Use transactions appropriately
- âœ… Monitor for connection leaks

### Streaming Best Practices

#### DO:
1. **Always close streams**
   ```java
   try (Stream<EnhancedDetailRecord> stream = repo.streamJsonbRecords(masterId)) {
       stream.forEach(this::process);
   }
   ```

2. **Use appropriate fetch sizes**
   - Small data: 100-500
   - Medium data: 1000-5000
   - Large data: 5000-10000

3. **Log progress periodically**
   ```java
   if (count % 10000 == 0) {
       log.info("Processed {} records", count);
   }
   ```

4. **Handle errors gracefully**
   ```java
   stream.forEach(record -> {
       try {
           process(record);
       } catch (Exception e) {
           log.error("Failed to process record {}", record.getId(), e);
           // Continue processing
       }
   });
   ```

#### DON'T:
1. **Don't collect streams to lists**
   ```java
   // BAD - defeats streaming purpose
   List<EnhancedDetailRecord> all = stream.collect(Collectors.toList());
   ```

2. **Don't create multiple streams per master**
   ```java
   // BAD - opens multiple cursors
   Stream<EnhancedDetailRecord> stream1 = repo.stream(masterId);
   Stream<EnhancedDetailRecord> stream2 = repo.stream(masterId);
   ```

3. **Don't use parallel streams unnecessarily**
   ```java
   // Usually not needed and can cause issues
   stream.parallel().forEach(...)
   ```

### File Processing
- âœ… Use BeanIO for structured file formats
- âœ… Buffer file writes for performance
- âœ… Handle large files with streaming
- âœ… Validate data before writing
- âœ… Use meaningful file names with timestamps
- âœ… Implement file rotation if needed

### Error Handling
- âœ… Catch specific exceptions
- âœ… Log errors with context
- âœ… Update status to `FAILED` on errors
- âœ… Record error messages in database
- âœ… Implement retry logic where appropriate
- âœ… Don't swallow exceptions silently

### Monitoring
- âœ… Enable Spring Boot Actuator
- âœ… Log key processing milestones
- âœ… Monitor database locks and connections
- âœ… Track processing throughput
- âœ… Set up alerts for failures
- âœ… Review logs regularly

---

## Advanced Topics

### Custom JSONB Type Handlers

Create custom type handlers for complex JSONB structures:

```java
public class CustomJsonbTypeHandler implements TypeHandler {
    private final ObjectMapper objectMapper;
    
    @Override
    public Object getResult(ResultSet rs, String columnName) throws SQLException {
        PGobject pgo = (PGobject) rs.getObject(columnName);
        if (pgo == null) return null;
        
        try {
            return objectMapper.readValue(pgo.getValue(), CustomType.class);
        } catch (JsonProcessingException e) {
            throw new SQLException("Failed to parse JSONB", e);
        }
    }
}
```

### Parallel Streaming

Split work across partitions for parallel processing:

```java
// Split work across partitions
int partitionCount = 4;
IntStream.range(0, partitionCount).parallel().forEach(partition -> {
    Stream<EnhancedDetailRecord> stream = 
        repository.streamPartition(masterId, partition, partitionCount);
    
    // Process partition
    processStream(stream);
});
```

### Custom JSONB Queries

Leverage PostgreSQL's powerful JSONB operators:

```java
String sql = """
    SELECT detail_id, transaction_data, amount
    FROM enhanced_detail_records
    WHERE master_id = ?
      AND transaction_data @> '{"status": "COMPLETED"}'::jsonb
      AND (transaction_data->>'amount')::NUMERIC > 1000
      AND transaction_data ? 'risk_score'
    ORDER BY (transaction_data->>'risk_score')::NUMERIC DESC
    """;

Stream<EnhancedDetailRecord> stream = 
    repository.streamWithCustomQuery(sql, 1000, masterId);
```

### Streaming Transformations

Apply transformations to streams efficiently:

```java
stream
    .filter(r -> r.getTransactionData() != null)
    .filter(r -> r.getTransactionData().getRiskScore() > 50)
    .map(this::enrichRecord)
    .map(this::flattenRecord)
    .forEach(writer::write);
```

### Custom BeanIO Mappings

Define custom field handlers:

```java
public class CurrencyTypeHandler implements TypeHandler {
    @Override
    public String format(Object value) {
        if (value instanceof Currency) {
            return ((Currency) value).getCurrencyCode();
        }
        return null;
    }
    
    @Override
    public Object parse(String text) {
        return Currency.getInstance(text);
    }
}
```

### Compression for Large Files

Enable GZIP compression for output files:

```java
public Path writeCompressedFile(MasterRecord master, 
                                Stream<DetailRecord> details) {
    Path output = resolveOutputPath(master);
    
    try (GZIPOutputStream gzipStream = new GZIPOutputStream(
            Files.newOutputStream(output));
         BeanWriter writer = createWriter(gzipStream)) {
        
        writeHeader(writer, master);
        details.forEach(detail -> writer.write("detail", detail));
        writeTrailer(writer, stats);
    }
    
    return output;
}
```

---

## Contributing

Contributions are welcome! Please follow these guidelines:

### Getting Started

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Write/update tests
5. Ensure tests pass (`./gradlew test`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Style

- Follow existing code conventions
- Use Spotless for formatting: `./gradlew spotlessApply`
- Run Detekt for static analysis: `./gradlew detekt`
- Maintain test coverage above 80%

### Pull Request Process

1. Update README.md with details of changes if applicable
2. Update CHANGELOG.md with notable changes
3. Ensure all tests pass
4. Request review from maintainers
5. Squash commits before merging

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## Appendix

### Performance Benchmarks

**Test Environment**:
- Dataset: 10 million rows with 5KB JSONB each
- Total Data: ~50GB
- Server: YugabyteDB 3-node cluster
- Client: 4 CPU cores, 8GB RAM

**Results (Streaming JSONB)**:

| Fetch Size | Memory Usage | Processing Time | Throughput |
|-----------|--------------|----------------|------------|
| 100 | 55MB | 45 min | 3,700 rec/sec |
| 1,000 | 60MB | 25 min | 6,600 rec/sec |
| 5,000 | 90MB | 18 min | 9,200 rec/sec |
| 10,000 | 150MB | 15 min | 11,100 rec/sec |

### Useful SQL Queries

```sql
-- Top business centers by volume
SELECT 
    business_center_code,
    COUNT(*) as total_records,
    SUM(CASE WHEN status = 'COMPLETED' THEN 1 ELSE 0 END) as completed,
    SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed,
    AVG(priority) as avg_priority
FROM master_records
GROUP BY business_center_code
ORDER BY total_records DESC;

-- Processing time distribution
SELECT 
    business_center_code,
    AVG(EXTRACT(EPOCH FROM (updated_at - locked_at))) as avg_seconds,
    MIN(EXTRACT(EPOCH FROM (updated_at - locked_at))) as min_seconds,
    MAX(EXTRACT(EPOCH FROM (updated_at - locked_at))) as max_seconds
FROM master_records
WHERE status = 'COMPLETED'
  AND locked_at IS NOT NULL
  AND updated_at IS NOT NULL
GROUP BY business_center_code;

-- Detail records per master distribution
SELECT 
    master_id,
    COUNT(*) as detail_count,
    SUM(amount) as total_amount
FROM detail_records
GROUP BY master_id
ORDER BY detail_count DESC
LIMIT 10;
```

### Environment Variables

The application supports configuration via environment variables:

```bash
# Database
export SPRING_DATASOURCE_URL=jdbc:postgresql://localhost:5433/yugabyte
export SPRING_DATASOURCE_USERNAME=yugabyte
export SPRING_DATASOURCE_PASSWORD=yugabyte

# Processing
export PROCESSOR_BATCH_SIZE=1000
export PROCESSOR_MAX_CONCURRENT_MASTERS=10
export PROCESSOR_OUTPUT_DIRECTORY=/app/output

# Server
export SERVER_PORT=8080

# Run application
java -jar yugabyte-file-processor-1.0.0.jar
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM eclipse-temurin:21-jre-alpine

WORKDIR /app

COPY build/libs/yugabyte-file-processor-1.0.0.jar app.jar

RUN mkdir -p /app/output && chmod 755 /app/output

EXPOSE 8080

ENTRYPOINT ["java", "-jar", "app.jar"]
```

```bash
# Build Docker image
docker build -t yugabyte-file-processor:latest .

# Run container
docker run -d \
  --name file-processor \
  -p 8080:8080 \
  -v $(pwd)/output:/app/output \
  -e SPRING_DATASOURCE_URL=jdbc:postgresql://yugabyte:5433/yugabyte \
  yugabyte-file-processor:latest
```

---

**For detailed streaming JSONB documentation, see [STREAMING_JSONB_GUIDE.md](STREAMING_JSONB_GUIDE.md)**

**For quick setup instructions, see [QUICKSTART.md](QUICKSTART.md)**
